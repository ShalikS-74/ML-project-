# ğŸ“š Exam Paper Intelligence & Trend Modeling Engine

A sophisticated ML system that analyzes exam papers to identify question patterns, detect duplicates, and model trending topics.

## ğŸ¯ Project Overview

**Problem**: Exam setters need to understand question patterns across multiple papers to ensure balanced coverage and avoid repetition.

**Solution**: ML-powered system that:
- Extracts questions from PDF papers
- Detects similar questions using embeddings
- Groups questions by similarity 
- Analyzes trending topics with weighted importance
- Generates professional reports

## ğŸ— System Architecture

```
Input PDFs â†’ Text Extraction â†’ Question Detection â†’ ML Embeddings â†’ Clustering â†’ Trend Analysis â†’ Report Generation
```

### Core Components
- **PDF Processor**: OCR + text extraction
- **Question Extractor**: Regex-based question detection
- **Embedding Engine**: SentenceTransformers for semantic similarity
- **Clustering Engine**: Agglomerative clustering with cosine similarity  
- **Report Generator**: Professional PDF output with trends

## ğŸš€ Quick Start

### Installation
```bash
cd exam_intelligence
pip install -r requirements.txt
```

### Basic Usage
```bash
# Week 1 MVP - Exact duplicate detection
python main.py --input_dir data/raw --mode extract

# Week 2+ - ML-based similarity  
python main.py --input_dir data/raw --mode full --tune_threshold
```

### Input Format
Place PDF files in `data/raw/`:
```
data/raw/
â”œâ”€â”€ paper1.pdf
â”œâ”€â”€ paper2.pdf
â””â”€â”€ paper3.pdf
```

### Output
- Console results (Week 1)
- Professional PDF reports (Week 2+)
- JSON data export
- Trend analysis charts

## ğŸ“… Development Timeline

### Week 1: MVP Foundation
**Goal**: Working system with exact duplicate detection
- âœ… PDF text extraction
- âœ… Regex question detection  
- âœ… Exact text matching
- âœ… Console output
- âœ… Basic frequency counting

**Deliverable**: Console app that finds identical questions

### Week 2: Add Intelligence  
**Goal**: ML-based similarity detection
- âœ… Sentence embeddings (all-MiniLM-L6-v2)
- âœ… Cosine similarity clustering
- âœ… Threshold tuning (0.75-0.85)
- âœ… Topic keyword extraction (TF-IDF)
- âœ… Cluster statistics

**Deliverable**: Intelligent question grouping

### Week 3: Professional Polish
**Goal**: Production-ready system
- âœ… Professional PDF reporting
- âœ… Weighted trend analysis  
- âœ… Diagram question detection
- âœ… Performance optimization
- âœ… Simple UI (optional)

**Deliverable**: Complete solution ready for real use

## ğŸ”§ Technical Stack

### Core ML
- **sentence-transformers**: Semantic embeddings
- **scikit-learn**: Clustering algorithms
- **numpy/pandas**: Data processing

### PDF Processing  
- **pdfplumber**: Primary text extraction
- **PyPDF2**: Fallback extraction
- **pytesseract**: OCR for scanned documents

### Output Generation
- **reportlab**: Professional PDF reports
- **matplotlib/seaborn**: Trend visualization
- **streamlit**: Optional web UI

### Development
- **pytest**: Testing framework
- **logging**: Comprehensive logging
- **pathlib**: Modern path handling

## ğŸ“Š Technical Decisions

### Why This Architecture?
1. **Local ML**: No API dependencies, handles exam season spikes
2. **Incremental Development**: MVP first, add complexity gradually  
3. **Modular Design**: Each component can be improved independently
4. **Academic Rigor**: Explainable similarity metrics and thresholds

### Key ML Choices
- **Embeddings**: all-MiniLM-L6-v2 (384 dim, good performance/size balance)
- **Similarity**: Cosine similarity (standard for embeddings)
- **Clustering**: Agglomerative with distance threshold (more interpretable than DBSCAN)
- **Threshold**: 0.75-0.85 range (tunable based on domain)

## ğŸ“ Project Structure

```
exam_intelligence/
â”œâ”€â”€ main.py                     # Entry point
â”œâ”€â”€ config.py                   # Configuration
â”œâ”€â”€ requirements.txt            # Dependencies
â”œâ”€â”€ WEEK1_PROMPTS.md           # Week 1 implementation guide
â”œâ”€â”€ WEEK2_3_PROMPTS.md         # Week 2-3 enhancement guide
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                   # Input PDFs
â”‚   â””â”€â”€ processed/             # Cached embeddings
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ preprocessing/
â”‚   â”‚   â”œâ”€â”€ pdf_processor.py   # PDF text extraction
â”‚   â”‚   â””â”€â”€ question_extractor.py # Question detection
â”‚   â”œâ”€â”€ ml_core/
â”‚   â”‚   â”œâ”€â”€ embedding_engine.py    # Sentence embeddings  
â”‚   â”‚   â”œâ”€â”€ clustering_engine.py   # Similarity clustering
â”‚   â”‚   â””â”€â”€ simple_matcher.py      # Week 1 exact matching
â”‚   â””â”€â”€ output/
â”‚       â”œâ”€â”€ report_generator.py    # PDF reports
â”‚       â””â”€â”€ console_output.py      # Console display
â”œâ”€â”€ tests/                     # Test suite
â”œâ”€â”€ notebooks/                 # Jupyter analysis
â””â”€â”€ outputs/                   # Generated reports
```

## ğŸ§ª Testing & Validation

### Week 1 Testing
```bash
# Test basic extraction
python -m pytest tests/test_extraction.py

# Manual validation
python main.py --input_dir test_data --mode extract
```

### Week 2+ Testing  
```bash
# Test ML pipeline
python -m pytest tests/test_ml_core.py

# Threshold tuning
python main.py --input_dir data/raw --tune_threshold

# Performance benchmarks  
python scripts/benchmark.py
```

### Validation Metrics
- **Precision**: % of grouped questions that are actually similar
- **Recall**: % of similar questions that are grouped together  
- **Efficiency**: Processing time per question
- **User Acceptance**: Manual review of clustering quality

## ğŸ“ Academic Rigor

This system demonstrates real ML engineering:

### Explainable Decisions
- Why cosine similarity? (Standard for embeddings, interpretable 0-1 range)
- Why threshold X? (Tuned on validation data with precision/recall curves)  
- How does OCR noise affect clustering? (Text normalization strategies)
- Why Agglomerative over DBSCAN? (More stable, interpretable distance threshold)

### Evaluation Strategy
- Manual labeling of 20 similar + 20 dissimilar question pairs
- Precision/recall on labeled data
- False positive (wrong merges) and false negative (missed similarities) analysis
- Ablation studies on preprocessing steps

### Scalability Considerations  
- Batch embedding generation
- Embedding caching for repeated use
- Memory-efficient similarity computation
- Incremental clustering for new papers

## ğŸ“ˆ Future Enhancements

### Advanced Features (Post-MVP)
- **Deep Question Understanding**: Fine-tuned BERT for exam domain
- **Multi-modal Analysis**: Handle questions with images/diagrams
- **Temporal Trends**: Track topic evolution across years
- **Difficulty Prediction**: ML model for question difficulty
- **Automated Question Generation**: Generate similar questions

### Integration Options
- **LMS Integration**: Connect with exam management systems
- **API Service**: REST API for institutional use  
- **Batch Processing**: Handle hundreds of papers
- **Real-time Analysis**: Live question similarity checking

## ğŸ¤ Contributing

### Implementation Guide
1. **Week 1**: Use `WEEK1_PROMPTS.md` for MVP implementation
2. **Week 2-3**: Use `WEEK2_3_PROMPTS.md` for enhancements  
3. **Testing**: Run test suite before submitting
4. **Documentation**: Update README for new features

### Code Standards
- Type hints for public functions
- Comprehensive logging
- Error handling for all file operations
- Docstrings for complex algorithms

## ğŸ“„ License

MIT License - Free for educational and commercial use.

## ğŸ¯ Success Metrics

### Technical Success
- âœ… Processes 4 papers in <30 seconds
- âœ… >80% accuracy on similar question detection  
- âœ… <2GB memory usage for 200 questions
- âœ… Professional PDF generation in <10 seconds

### User Success  
- âœ… Reduces manual pattern analysis from hours to minutes
- âœ… Discovers non-obvious question similarities
- âœ… Provides actionable insights for exam improvement
- âœ… Produces reports suitable for institutional review

### Academic Success
- âœ… Demonstrates ML engineering best practices
- âœ… Explainable similarity metrics and clustering decisions
- âœ… Rigorous evaluation methodology
- âœ… Scalable and maintainable architecture

---

**Ready to build intelligent exam analysis? Start with Week 1 MVP! ğŸš€**